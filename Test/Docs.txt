Hello sivakumar
Hello Canvas
Hello India

import org.apache.beam.sdk.transforms.DoFn;
import org.apache.beam.sdk.transforms.ParDo;
import org.apache.beam.sdk.transforms.PTransform;
import org.apache.beam.sdk.values.PCollection;
import org.apache.beam.sdk.transforms.MapElements;
import org.apache.beam.sdk.values.TypeDescriptors;

public class GreetingTransform extends PTransform<PCollection<String>, PCollection<String>> {

    @Override
    public PCollection<String> expand(PCollection<String> input) {

        return input
            // Remove header row (where first column is "id")
            .apply("SkipHeader", ParDo.of(new DoFn<String, String>() {
                private boolean headerSkipped = false;

                @ProcessElement
                public void processElement(@Element String line, OutputReceiver<String> out) {
                    String[] parts = line.split(",", -1);
                    if (!headerSkipped && parts.length > 0 && parts[0].trim().equalsIgnoreCase("id")) {
                        headerSkipped = true;
                        return;
                    }
                    out.output(line);
                }
            }))

            // Filter valid rows: numeric ID and role == Manager
            .apply("FilterValidManagers", ParDo.of(new DoFn<String, String>() {
                @ProcessElement
                public void processElement(@Element String line, OutputReceiver<String> out) {
                    String[] parts = line.split(",", -1);
                    if (parts.length < 3) return;

                    String id = parts[0].trim();
                    String role = parts[2].trim();

                    if (!id.isEmpty() && id.matches("\\d+") && role.equalsIgnoreCase("Manager")) {
                        out.output(line);
                    }
                }
            }))



import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.io.TextIO;
import org.apache.beam.sdk.options.PipelineOptionsFactory;
import org.apache.beam.sdk.transforms.DoFn;
import org.apache.beam.sdk.transforms.ParDo;
import org.apache.beam.sdk.transforms.Create;
import org.apache.beam.sdk.values.PCollection;

import java.util.Arrays;
import java.util.List;

public class CsvHeaderTrailerValidator {

    static final List<String> HEADER_SCHEMA = Arrays.asList("Hid", "name", "sal");
    static final List<String> TRAILER_SCHEMA = Arrays.asList("Tid", "Record", "count");

    public static void main(String[] args) {
        Pipeline pipeline = Pipeline.create(
                PipelineOptionsFactory.fromArgs(args).withValidation().create());

        PCollection<String> lines = pipeline.apply("ReadCSV", TextIO.read().from("input.csv"));

        lines.apply("ValidateHeaderTrailer", ParDo.of(new DoFn<String, String>() {
            boolean headerValidated = false;

            @ProcessElement
            public void processElement(ProcessContext c) {
                String line = c.element();

                if (line.startsWith("H")) {
                    List<String> headerFields = Arrays.asList(line.split(","));
                    if (validateSchema(headerFields, HEADER_SCHEMA)) {
                        headerValidated = true;
                        System.out.println("Header valid: " + line);
                    } else {
                        System.err.println("Invalid Header: " + line);
                    }
                } else if (line.startsWith("T")) {
                    List<String> trailerFields = Arrays.asList(line.split(","));
                    if (validateSchema(trailerFields, TRAILER_SCHEMA)) {
                        System.out.println("Trailer valid: " + line);
                    } else {
                        System.err.println("Invalid Trailer: " + line);
                    }
                } else {
                    // Data lines (non-header, non-trailer)
                    if (headerValidated) {
                        c.output(line);
                    } else {
                        System.err.println("Data found before valid header: " + line);
                    }
                }
            }

            private boolean validateSchema(List<String> actual, List<String> expected) {
                if (actual.size() != expected.size()) return false;
                for (int i = 0; i < actual.size(); i++) {
                    if (!actual.get(i).trim().equalsIgnoreCase(expected.get(i))) {
                        return false;
                    }
                }
                return true;
            }
        })).apply("WriteValidData", TextIO.write().to("output").withSuffix(".txt").withoutSharding());

        pipeline.run().waitUntilFinish();
    }
}


            // Generate greeting using second column (name)
            .apply("CreateGreetings", MapElements
                .into(TypeDescriptors.strings())
                .via((String line) -> {
                    String[] parts = line.split(",", -1);
                    String name = parts[1].trim();
                    return "Hello, " + name + "!";
                }));
    }
}


import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.io.TextIO;
import org.apache.beam.sdk.io.parquet.ParquetIO;
import org.apache.beam.sdk.transforms.DoFn;
import org.apache.beam.sdk.transforms.ParDo;
import org.apache.beam.sdk.values.PCollection;
import org.apache.parquet.example.data.simple.SimpleGroup;
import org.apache.parquet.schema.MessageType;
import org.apache.parquet.schema.OriginalType;
import org.apache.parquet.schema.PrimitiveType;
import org.apache.parquet.schema.Types;

public class CsvToParquet {

    public static void main(String[] args) {

        // Define Parquet schema
        MessageType schema = Types.buildMessage()
            .required(PrimitiveType.PrimitiveTypeName.BINARY).as(OriginalType.UTF8).named("name")
            .required(PrimitiveType.PrimitiveTypeName.INT32).named("age")
            .named("Person");

        Pipeline pipeline = Pipeline.create();

        // Read CSV lines
        PCollection<String> lines = pipeline.apply("Read CSV", TextIO.read().from("input.csv"));

        // Parse CSV and convert to SimpleGroup
        PCollection<SimpleGroup> records = lines.apply("Parse CSV", ParDo.of(new DoFn<String, SimpleGroup>() {
            private boolean headerSkipped = false;

            @ProcessElement
            public void processElement(ProcessContext c) {
                String line = c.element();
                if (!headerSkipped && line.contains("name")) {
                    headerSkipped = true;
                    return; // skip header
                }

                String[] parts = line.split(",");
                if (parts.length >= 2) {
                    SimpleGroup group = new SimpleGroup(schema);
                    group.add("name", parts[0]);
                    group.add("age", Integer.parseInt(parts[1]));
                    c.output(group);




package com.example.pipeline.config;

import org.apache.beam.sdk.io.jdbc.JdbcIO;

public class DbConfig {
    public static JdbcIO.DataSourceConfiguration getPostgresConfig() {
        return JdbcIO.DataSourceConfiguration.create(
                    "org.postgresql.Driver",
                    "jdbc:postgresql://localhost:5432/your_db")
                .withUsername("your_username")
                .withPassword("your_password");
    }
}




                }
            }
        }));

        // Write to Parquet
        records.apply("Write to Parquet", ParquetIO.write(schema)
                .to("output/people")        // Output will be people-00000-of-00001.parquet
                .withSuffix(".parquet"));

        pipeline.run().waitUntilFinish();
    }
}

package com.example.pipeline;

import com.example.pipeline.model.User;
import com.example.pipeline.config.DbConfig;

import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.io.jdbc.JdbcIO;
import org.apache.beam.sdk.io.parquet.ParquetIO;
import org.apache.beam.sdk.transforms.MapElements;
import org.apache.beam.sdk.transforms.SimpleFunction;
import org.apache.parquet.example.data.Group;
import org.apache.parquet.schema.MessageTypeParser;

public class ParquetToPostgres {
    public static void main(String[] args) {
        Pipeline pipeline = Pipeline.create();

        String parquetSchema = "message user { "
                + "required int64 id; "
                + "required binary name (UTF8); "
                + "required binary email (UTF8); "
                + "}";

        pipeline.apply("ReadParquet",
                ParquetIO.read(Group.class)
                        .withSchema(MessageTypeParser.parseMessageType(parquetSchema))
                        .from("path/to/user.parquet"))

                .apply("GroupToUser", MapElements.via(new SimpleFunction<Group, User>() {
                    public User apply(Group group) {
                        return new User(
                                group.getLong("id", 0),
                                group.getString("name", 0),
                                group.getString("email", 0)
                        );
                    }
                }))

                .apply("WriteToPostgres", JdbcIO.<User>write()
                        .withDataSourceConfiguration(DbConfig.getPostgresConfig())
                        .withBatchSize(1000)  // Efficient batching
                        .withStatement("INSERT INTO users (id, name, email) VALUES (?, ?, ?)")
                        .withPreparedStatementSetter((user, ps) -> {
                            ps.setLong(1, user.getId());
                            ps.setString(2, user.getName());
                            ps.setString(3, user.getEmail());
                        }));

        pipeline.run().waitUntilFinish();
    }
}



package com.example.pipeline.config;

import org.apache.beam.sdk.io.jdbc.JdbcIO;

import java.util.Properties;

public class DbConfig {

    public static JdbcIO.DataSourceConfiguration getPostgresConfig() {
        Properties properties = new Properties();

        // Custom JDBC properties
        properties.setProperty("reWriteBatchedInserts", "true");  // Required for PostgreSQL batch efficiency
        properties.setProperty("ssl", "false");  // Enable if using SSL
        properties.setProperty("connectTimeout", "10");  // in seconds
        properties.setProperty("socketTimeout", "60");   // in seconds
        properties.setProperty("tcpKeepAlive", "true");  // maintain idle connections

        return JdbcIO.DataSourceConfiguration.create(
                        "org.postgresql.Driver",
                        "jdbc:postgresql://localhost:5432/your_db")
                .withUsername("your_username")
                .withPassword("your_password")
                .withConnectionProperties(properties);
    }
}



import java.io.IOException;
import java.util.logging.*;

public class ErrorLogFileApp {

    private static final Logger logger = Logger.getLogger(ErrorLogFileApp.class.getName());

    public static void main(String[] args) {
        setupErrorLogger();

        logger.info("Application started.");

        try {
            simulateBusinessLogic();
        } catch (Exception e) {
            logger.log(Level.SEVERE, "Exception caught in main method", e);
        }

        logger.info("Application finished.");
    }

    private static void setupErrorLogger() {
        try {
            // Create error log file at the start of the program
            Handler fileHandler = new FileHandler("error-log.txt", true); // true = append to existing
            fileHandler.setFormatter(new SimpleFormatter());
            fileHandler.setLevel(Level.ALL);

            logger.addHandler(fileHandler);
            logger.setLevel(Level.ALL);
            logger.setUseParentHandlers(false); // Avoid logging to console

            logger.info("Error log file initialized.");

        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    private static void simulateBusinessLogic() {
        logger.info("Simulating logic...");

        // Simulate error
        String data = null;
        int length = data.length(); // This will throw NullPointerException

        logger.info("This line won't execute due to exception above.");
    }
}



<dependency>
  <groupId>org.apache.poi</groupId>
  <artifactId>poi</artifactId>
  <version>5.3.0</version>
  <exclusions>
    <exclusion>
      <groupId>org.apache.logging.log4j</groupId>
      <artifactId>log4j-api</artifactId>
    </exclusion>
  </exclusions>
</dependency>

<dependency>
  <groupId>org.apache.hadoop</groupId>
  <artifactId>hadoop-common</artifactId>
  <version>3.3.6</version>
  <exclusions>
    <exclusion>
      <groupId>log4j</groupId>
      <artifactId>log4j</artifactId>
    </exclusion>
    <exclusion>
      <groupId>org.slf4j</groupId>
      <artifactId>slf4j-log4j12</artifactId>
    </exclusion>
  </exclusions>
</dependency>




public static void truncateAndInsert(String tableName, List<YourDataModel> dataList) {
    Connection connection = null;
    PreparedStatement truncateStatement = null;
    PreparedStatement insertStatement = null;

    try {
        JdbcIO.DataSourceConfiguration postgresConfig = PostgresConfig.getPostgresConfig();
        connection = postgresConfig.buildDataSource().getConnection();
        connection.setAutoCommit(false); // Begin transaction

        // Step 1: Truncate table
        String truncateQuery = "TRUNCATE TABLE " + tableName;
        truncateStatement = connection.prepareStatement(truncateQuery);
        truncateStatement.executeUpdate();
        LOG.info("Table truncated: {}", tableName);

        // Step 2: Insert data
        String insertQuery = "INSERT INTO " + tableName + " (column1, column2) VALUES (?, ?)";
        insertStatement = connection.prepareStatement(insertQuery);

        for (YourDataModel item : dataList) {
            insertStatement.setString(1, item.getColumn1());
            insertStatement.setInt(2, item.getColumn2());
            insertStatement.addBatch();
        }

        int[] result = insertStatement.executeBatch();
        LOG.info("Inserted rows: {}", Arrays.stream(result).sum());

        connection.commit(); // Commit if all goes well
    } catch (Exception e) {
        LOG.error("Error during truncate and insert: {}", e.getMessage());
        if (connection != null) {
            try {
                connection.rollback();
                LOG.warn("Transaction rolled back.");
            } catch (SQLException rollbackEx) {
                LOG.error("Failed to rollback: {}", rollbackEx.getMessage());
            }
        }
    } finally {
        try {
            if (truncateStatement != null) truncateStatement.close();
            if (insertStatement != null) insertStatement.close();
            if (connection != null) connection.close();
        } catch (SQLException closeEx) {
            LOG.warn("Failed to close DB resources: {}", closeEx.getMessage());
        }
    }
}
