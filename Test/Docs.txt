Hello sivakumar
Hello Canvas
Hello India

import org.apache.beam.sdk.transforms.DoFn;
import org.apache.beam.sdk.transforms.ParDo;
import org.apache.beam.sdk.transforms.PTransform;
import org.apache.beam.sdk.values.PCollection;
import org.apache.beam.sdk.transforms.MapElements;
import org.apache.beam.sdk.values.TypeDescriptors;

public class GreetingTransform extends PTransform<PCollection<String>, PCollection<String>> {

    @Override
    public PCollection<String> expand(PCollection<String> input) {

        return input
            // Remove header row (where first column is "id")
            .apply("SkipHeader", ParDo.of(new DoFn<String, String>() {
                private boolean headerSkipped = false;

                @ProcessElement
                public void processElement(@Element String line, OutputReceiver<String> out) {
                    String[] parts = line.split(",", -1);
                    if (!headerSkipped && parts.length > 0 && parts[0].trim().equalsIgnoreCase("id")) {
                        headerSkipped = true;
                        return;
                    }
                    out.output(line);
                }
            }))

            // Filter valid rows: numeric ID and role == Manager
            .apply("FilterValidManagers", ParDo.of(new DoFn<String, String>() {
                @ProcessElement
                public void processElement(@Element String line, OutputReceiver<String> out) {
                    String[] parts = line.split(",", -1);
                    if (parts.length < 3) return;

                    String id = parts[0].trim();
                    String role = parts[2].trim();

                    if (!id.isEmpty() && id.matches("\\d+") && role.equalsIgnoreCase("Manager")) {
                        out.output(line);
                    }
                }
            }))



import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.io.TextIO;
import org.apache.beam.sdk.options.PipelineOptionsFactory;
import org.apache.beam.sdk.transforms.DoFn;
import org.apache.beam.sdk.transforms.ParDo;
import org.apache.beam.sdk.transforms.Create;
import org.apache.beam.sdk.values.PCollection;

import java.util.Arrays;
import java.util.List;

public class CsvHeaderTrailerValidator {

    static final List<String> HEADER_SCHEMA = Arrays.asList("Hid", "name", "sal");
    static final List<String> TRAILER_SCHEMA = Arrays.asList("Tid", "Record", "count");

    public static void main(String[] args) {
        Pipeline pipeline = Pipeline.create(
                PipelineOptionsFactory.fromArgs(args).withValidation().create());

        PCollection<String> lines = pipeline.apply("ReadCSV", TextIO.read().from("input.csv"));

        lines.apply("ValidateHeaderTrailer", ParDo.of(new DoFn<String, String>() {
            boolean headerValidated = false;

            @ProcessElement
            public void processElement(ProcessContext c) {
                String line = c.element();

                if (line.startsWith("H")) {
                    List<String> headerFields = Arrays.asList(line.split(","));
                    if (validateSchema(headerFields, HEADER_SCHEMA)) {
                        headerValidated = true;
                        System.out.println("Header valid: " + line);
                    } else {
                        System.err.println("Invalid Header: " + line);
                    }
                } else if (line.startsWith("T")) {
                    List<String> trailerFields = Arrays.asList(line.split(","));
                    if (validateSchema(trailerFields, TRAILER_SCHEMA)) {
                        System.out.println("Trailer valid: " + line);
                    } else {
                        System.err.println("Invalid Trailer: " + line);
                    }
                } else {
                    // Data lines (non-header, non-trailer)
                    if (headerValidated) {
                        c.output(line);
                    } else {
                        System.err.println("Data found before valid header: " + line);
                    }
                }
            }

            private boolean validateSchema(List<String> actual, List<String> expected) {
                if (actual.size() != expected.size()) return false;
                for (int i = 0; i < actual.size(); i++) {
                    if (!actual.get(i).trim().equalsIgnoreCase(expected.get(i))) {
                        return false;
                    }
                }
                return true;
            }
        })).apply("WriteValidData", TextIO.write().to("output").withSuffix(".txt").withoutSharding());

        pipeline.run().waitUntilFinish();
    }
}


            // Generate greeting using second column (name)
            .apply("CreateGreetings", MapElements
                .into(TypeDescriptors.strings())
                .via((String line) -> {
                    String[] parts = line.split(",", -1);
                    String name = parts[1].trim();
                    return "Hello, " + name + "!";
                }));
    }
}


import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.io.TextIO;
import org.apache.beam.sdk.io.parquet.ParquetIO;
import org.apache.beam.sdk.transforms.DoFn;
import org.apache.beam.sdk.transforms.ParDo;
import org.apache.beam.sdk.values.PCollection;
import org.apache.parquet.example.data.simple.SimpleGroup;
import org.apache.parquet.schema.MessageType;
import org.apache.parquet.schema.OriginalType;
import org.apache.parquet.schema.PrimitiveType;
import org.apache.parquet.schema.Types;

public class CsvToParquet {

    public static void main(String[] args) {

        // Define Parquet schema
        MessageType schema = Types.buildMessage()
            .required(PrimitiveType.PrimitiveTypeName.BINARY).as(OriginalType.UTF8).named("name")
            .required(PrimitiveType.PrimitiveTypeName.INT32).named("age")
            .named("Person");

        Pipeline pipeline = Pipeline.create();

        // Read CSV lines
        PCollection<String> lines = pipeline.apply("Read CSV", TextIO.read().from("input.csv"));

        // Parse CSV and convert to SimpleGroup
        PCollection<SimpleGroup> records = lines.apply("Parse CSV", ParDo.of(new DoFn<String, SimpleGroup>() {
            private boolean headerSkipped = false;

            @ProcessElement
            public void processElement(ProcessContext c) {
                String line = c.element();
                if (!headerSkipped && line.contains("name")) {
                    headerSkipped = true;
                    return; // skip header
                }

                String[] parts = line.split(",");
                if (parts.length >= 2) {
                    SimpleGroup group = new SimpleGroup(schema);
                    group.add("name", parts[0]);
                    group.add("age", Integer.parseInt(parts[1]));
                    c.output(group);
                }
            }
        }));

        // Write to Parquet
        records.apply("Write to Parquet", ParquetIO.write(schema)
                .to("output/people")        // Output will be people-00000-of-00001.parquet
                .withSuffix(".parquet"));

        pipeline.run().waitUntilFinish();
    }
}

