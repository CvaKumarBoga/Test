Hello sivakumar
Hello Canvas
Hello India

import org.apache.beam.sdk.transforms.DoFn;
import org.apache.beam.sdk.transforms.ParDo;
import org.apache.beam.sdk.transforms.PTransform;
import org.apache.beam.sdk.values.PCollection;
import org.apache.beam.sdk.transforms.MapElements;
import org.apache.beam.sdk.values.TypeDescriptors;

public class GreetingTransform extends PTransform<PCollection<String>, PCollection<String>> {

    @Override
    public PCollection<String> expand(PCollection<String> input) {

        return input
            // Remove header row (where first column is "id")
            .apply("SkipHeader", ParDo.of(new DoFn<String, String>() {
                private boolean headerSkipped = false;

                @ProcessElement
                public void processElement(@Element String line, OutputReceiver<String> out) {
                    String[] parts = line.split(",", -1);
                    if (!headerSkipped && parts.length > 0 && parts[0].trim().equalsIgnoreCase("id")) {
                        headerSkipped = true;
                        return;
                    }
                    out.output(line);
                }
            }))

            // Filter valid rows: numeric ID and role == Manager
            .apply("FilterValidManagers", ParDo.of(new DoFn<String, String>() {
                @ProcessElement
                public void processElement(@Element String line, OutputReceiver<String> out) {
                    String[] parts = line.split(",", -1);
                    if (parts.length < 3) return;

                    String id = parts[0].trim();
                    String role = parts[2].trim();

                    if (!id.isEmpty() && id.matches("\\d+") && role.equalsIgnoreCase("Manager")) {
                        out.output(line);
                    }
                }
            }))



import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.io.TextIO;
import org.apache.beam.sdk.options.PipelineOptionsFactory;
import org.apache.beam.sdk.transforms.DoFn;
import org.apache.beam.sdk.transforms.ParDo;
import org.apache.beam.sdk.transforms.Create;
import org.apache.beam.sdk.values.PCollection;

import java.util.Arrays;
import java.util.List;

public class CsvHeaderTrailerValidator {

    static final List<String> HEADER_SCHEMA = Arrays.asList("Hid", "name", "sal");
    static final List<String> TRAILER_SCHEMA = Arrays.asList("Tid", "Record", "count");

    public static void main(String[] args) {
        Pipeline pipeline = Pipeline.create(
                PipelineOptionsFactory.fromArgs(args).withValidation().create());

        PCollection<String> lines = pipeline.apply("ReadCSV", TextIO.read().from("input.csv"));

        lines.apply("ValidateHeaderTrailer", ParDo.of(new DoFn<String, String>() {
            boolean headerValidated = false;

            @ProcessElement
            public void processElement(ProcessContext c) {
                String line = c.element();

                if (line.startsWith("H")) {
                    List<String> headerFields = Arrays.asList(line.split(","));
                    if (validateSchema(headerFields, HEADER_SCHEMA)) {
                        headerValidated = true;
                        System.out.println("Header valid: " + line);
                    } else {
                        System.err.println("Invalid Header: " + line);
                    }
                } else if (line.startsWith("T")) {
                    List<String> trailerFields = Arrays.asList(line.split(","));
                    if (validateSchema(trailerFields, TRAILER_SCHEMA)) {
                        System.out.println("Trailer valid: " + line);
                    } else {
                        System.err.println("Invalid Trailer: " + line);
                    }
                } else {
                    // Data lines (non-header, non-trailer)
                    if (headerValidated) {
                        c.output(line);
                    } else {
                        System.err.println("Data found before valid header: " + line);
                    }
                }
            }

            private boolean validateSchema(List<String> actual, List<String> expected) {
                if (actual.size() != expected.size()) return false;
                for (int i = 0; i < actual.size(); i++) {
                    if (!actual.get(i).trim().equalsIgnoreCase(expected.get(i))) {
                        return false;
                    }
                }
                return true;
            }
        })).apply("WriteValidData", TextIO.write().to("output").withSuffix(".txt").withoutSharding());

        pipeline.run().waitUntilFinish();
    }
}


            // Generate greeting using second column (name)
            .apply("CreateGreetings", MapElements
                .into(TypeDescriptors.strings())
                .via((String line) -> {
                    String[] parts = line.split(",", -1);
                    String name = parts[1].trim();
                    return "Hello, " + name + "!";
                }));
    }
}


import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.io.TextIO;
import org.apache.beam.sdk.io.parquet.ParquetIO;
import org.apache.beam.sdk.transforms.DoFn;
import org.apache.beam.sdk.transforms.ParDo;
import org.apache.beam.sdk.values.PCollection;
import org.apache.parquet.example.data.simple.SimpleGroup;
import org.apache.parquet.schema.MessageType;
import org.apache.parquet.schema.OriginalType;
import org.apache.parquet.schema.PrimitiveType;
import org.apache.parquet.schema.Types;

public class CsvToParquet {

    public static void main(String[] args) {

        // Define Parquet schema
        MessageType schema = Types.buildMessage()
            .required(PrimitiveType.PrimitiveTypeName.BINARY).as(OriginalType.UTF8).named("name")
            .required(PrimitiveType.PrimitiveTypeName.INT32).named("age")
            .named("Person");

        Pipeline pipeline = Pipeline.create();

        // Read CSV lines
        PCollection<String> lines = pipeline.apply("Read CSV", TextIO.read().from("input.csv"));

        // Parse CSV and convert to SimpleGroup
        PCollection<SimpleGroup> records = lines.apply("Parse CSV", ParDo.of(new DoFn<String, SimpleGroup>() {
            private boolean headerSkipped = false;

            @ProcessElement
            public void processElement(ProcessContext c) {
                String line = c.element();
                if (!headerSkipped && line.contains("name")) {
                    headerSkipped = true;
                    return; // skip header
                }

                String[] parts = line.split(",");
                if (parts.length >= 2) {
                    SimpleGroup group = new SimpleGroup(schema);
                    group.add("name", parts[0]);
                    group.add("age", Integer.parseInt(parts[1]));
                    c.output(group);




package com.example.pipeline.config;

import org.apache.beam.sdk.io.jdbc.JdbcIO;

public class DbConfig {
    public static JdbcIO.DataSourceConfiguration getPostgresConfig() {
        return JdbcIO.DataSourceConfiguration.create(
                    "org.postgresql.Driver",
                    "jdbc:postgresql://localhost:5432/your_db")
                .withUsername("your_username")
                .withPassword("your_password");
    }
}




                }
            }
        }));

        // Write to Parquet
        records.apply("Write to Parquet", ParquetIO.write(schema)
                .to("output/people")        // Output will be people-00000-of-00001.parquet
                .withSuffix(".parquet"));

        pipeline.run().waitUntilFinish();
    }
}

package com.example.pipeline;

import com.example.pipeline.model.User;
import com.example.pipeline.config.DbConfig;

import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.io.jdbc.JdbcIO;
import org.apache.beam.sdk.io.parquet.ParquetIO;
import org.apache.beam.sdk.transforms.MapElements;
import org.apache.beam.sdk.transforms.SimpleFunction;
import org.apache.parquet.example.data.Group;
import org.apache.parquet.schema.MessageTypeParser;

public class ParquetToPostgres {
    public static void main(String[] args) {
        Pipeline pipeline = Pipeline.create();

        String parquetSchema = "message user { "
                + "required int64 id; "
                + "required binary name (UTF8); "
                + "required binary email (UTF8); "
                + "}";

        pipeline.apply("ReadParquet",
                ParquetIO.read(Group.class)
                        .withSchema(MessageTypeParser.parseMessageType(parquetSchema))
                        .from("path/to/user.parquet"))

                .apply("GroupToUser", MapElements.via(new SimpleFunction<Group, User>() {
                    public User apply(Group group) {
                        return new User(
                                group.getLong("id", 0),
                                group.getString("name", 0),
                                group.getString("email", 0)
                        );
                    }
                }))

                .apply("WriteToPostgres", JdbcIO.<User>write()
                        .withDataSourceConfiguration(DbConfig.getPostgresConfig())
                        .withBatchSize(1000)  // Efficient batching
                        .withStatement("INSERT INTO users (id, name, email) VALUES (?, ?, ?)")
                        .withPreparedStatementSetter((user, ps) -> {
                            ps.setLong(1, user.getId());
                            ps.setString(2, user.getName());
                            ps.setString(3, user.getEmail());
                        }));

        pipeline.run().waitUntilFinish();
    }
}



package com.example.pipeline.config;

import org.apache.beam.sdk.io.jdbc.JdbcIO;

import java.util.Properties;

public class DbConfig {

    public static JdbcIO.DataSourceConfiguration getPostgresConfig() {
        Properties properties = new Properties();

        // Custom JDBC properties
        properties.setProperty("reWriteBatchedInserts", "true");  // Required for PostgreSQL batch efficiency
        properties.setProperty("ssl", "false");  // Enable if using SSL
        properties.setProperty("connectTimeout", "10");  // in seconds
        properties.setProperty("socketTimeout", "60");   // in seconds
        properties.setProperty("tcpKeepAlive", "true");  // maintain idle connections

        return JdbcIO.DataSourceConfiguration.create(
                        "org.postgresql.Driver",
                        "jdbc:postgresql://localhost:5432/your_db")
                .withUsername("your_username")
                .withPassword("your_password")
                .withConnectionProperties(properties);
    }
}



import java.io.IOException;
import java.util.logging.*;

public class ErrorLogFileApp {

    private static final Logger logger = Logger.getLogger(ErrorLogFileApp.class.getName());

    public static void main(String[] args) {
        setupErrorLogger();

        logger.info("Application started.");

        try {
            simulateBusinessLogic();
        } catch (Exception e) {
            logger.log(Level.SEVERE, "Exception caught in main method", e);
        }

        logger.info("Application finished.");
    }

    private static void setupErrorLogger() {
        try {
            // Create error log file at the start of the program
            Handler fileHandler = new FileHandler("error-log.txt", true); // true = append to existing
            fileHandler.setFormatter(new SimpleFormatter());
            fileHandler.setLevel(Level.ALL);

            logger.addHandler(fileHandler);
            logger.setLevel(Level.ALL);
            logger.setUseParentHandlers(false); // Avoid logging to console

            logger.info("Error log file initialized.");

        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    private static void simulateBusinessLogic() {
        logger.info("Simulating logic...");

        // Simulate error
        String data = null;
        int length = data.length(); // This will throw NullPointerException

        logger.info("This line won't execute due to exception above.");
    }
}



<dependency>
  <groupId>org.apache.poi</groupId>
  <artifactId>poi</artifactId>
  <version>5.3.0</version>
  <exclusions>
    <exclusion>
      <groupId>org.apache.logging.log4j</groupId>
      <artifactId>log4j-api</artifactId>
    </exclusion>
  </exclusions>
</dependency>

<dependency>
  <groupId>org.apache.hadoop</groupId>
  <artifactId>hadoop-common</artifactId>
  <version>3.3.6</version>
  <exclusions>
    <exclusion>
      <groupId>log4j</groupId>
      <artifactId>log4j</artifactId>
    </exclusion>
    <exclusion>
      <groupId>org.slf4j</groupId>
      <artifactId>slf4j-log4j12</artifactId>
    </exclusion>
  </exclusions>
</dependency>




public static void truncateAndInsert(String tableName, List<YourDataModel> dataList) {
    Connection connection = null;
    PreparedStatement truncateStatement = null;
    PreparedStatement insertStatement = null;

    try {
        JdbcIO.DataSourceConfiguration postgresConfig = PostgresConfig.getPostgresConfig();
        connection = postgresConfig.buildDataSource().getConnection();
        connection.setAutoCommit(false); // Begin transaction

        // Step 1: Truncate table
        String truncateQuery = "TRUNCATE TABLE " + tableName;
        truncateStatement = connection.prepareStatement(truncateQuery);
        truncateStatement.executeUpdate();
        LOG.info("Table truncated: {}", tableName);

        // Step 2: Insert data
        String insertQuery = "INSERT INTO " + tableName + " (column1, column2) VALUES (?, ?)";
        insertStatement = connection.prepareStatement(insertQuery);

        for (YourDataModel item : dataList) {
            insertStatement.setString(1, item.getColumn1());
            insertStatement.setInt(2, item.getColumn2());
            insertStatement.addBatch();
        }

        int[] result = insertStatement.executeBatch();
        LOG.info("Inserted rows: {}", Arrays.stream(result).sum());

        connection.commit(); // Commit if all goes well
    } catch (Exception e) {
        LOG.error("Error during truncate and insert: {}", e.getMessage());
        if (connection != null) {
            try {
                connection.rollback();
                LOG.warn("Transaction rolled back.");
            } catch (SQLException rollbackEx) {
                LOG.error("Failed to rollback: {}", rollbackEx.getMessage());
            }
        }
    } finally {
        try {
            if (truncateStatement != null) truncateStatement.close();
            if (insertStatement != null) insertStatement.close();
            if (connection != null) connection.close();
        } catch (SQLException closeEx) {
            LOG.warn("Failed to close DB resources: {}", closeEx.getMessage());
        }
    }
}



<configuration>
    <!-- Customize directory and level via system property -->
    <property name="LOG_DIR" value="${log.dir:-logs}" />
    <property name="LOG_LEVEL" value="${log.level:-INFO}" />

    <!-- Console Appender -->
    <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n</pattern>
        </encoder>
    </appender>

    <!-- File Appender -->
    <appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>${LOG_DIR}/application.log</file>

        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${LOG_DIR}/application.%d{yyyy-MM-dd}.log</fileNamePattern>
            <maxHistory>10</maxHistory>
        </rollingPolicy>

        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss} %-5level %logger{36} - %msg%n</pattern>
        </encoder>
    </appender>

    <root level="${LOG_LEVEL}">
        <appender-ref ref="CONSOLE" />
        <appender-ref ref="FILE" />
    </root>
</configuration>




<configuration>

    <property name="LOG_DIR" value="${log.dir:-logs}" />

    <!-- Console Appender -->
    <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
        <encoder>
            <pattern>%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n</pattern>
        </encoder>
    </appender>

    <!-- General Application Log -->
    <appender name="APP_LOG" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>${LOG_DIR}/application.log</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${LOG_DIR}/application.%d{yyyy-MM-dd}.log</fileNamePattern>
        </rollingPolicy>
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss} %-5level %logger - %msg%n</pattern>
        </encoder>
    </appender>

    <!-- IO Exceptions Log -->
    <appender name="IO_EXCEPTIONS" class="ch.qos.logback.core.FileAppender">
        <file>${LOG_DIR}/io-exceptions.log</file>
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>ERROR</level>
        </filter>
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss} %-5level %logger - %msg%n</pattern>
        </encoder>
    </appender>

    <!-- General Exceptions Log -->
    <appender name="GENERIC_ERRORS" class="ch.qos.logback.core.FileAppender">
        <file>${LOG_DIR}/general-errors.log</file>
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>ERROR</level>
        </filter>
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss} %-5level %logger - %msg%n</pattern>
        </encoder>
    </appender>

    <!-- Root Logger -->
    <root level="INFO">
        <appender-ref ref="CONSOLE" />
        <appender-ref ref="APP_LOG" />
    </root>

    <!-- Logger for IOExceptions -->
    <logger name="io-exception-logger" level="ERROR" additivity="false">
        <appender-ref ref="IO_EXCEPTIONS" />
    </logger>

    <!-- Logger for General Errors -->
    <logger name="generic-error-logger" level="ERROR" additivity="false">
        <appender-ref ref="GENERIC_ERRORS" />
    </logger>

</configuration>



<configuration>

    <property name="LOG_DIR" value="${log.dir:-logs}" />

    <!-- Insert Job Logger -->
    <appender name="INSERT_FILE" class="ch.qos.logback.core.FileAppender">
        <file>${LOG_DIR}/insert-errors.log</file>
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n</pattern>
        </encoder>
    </appender>

    <!-- Read Job Logger -->
    <appender name="READ_FILE" class="ch.qos.logback.core.FileAppender">
        <file>${LOG_DIR}/read-errors.log</file>
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n</pattern>
        </encoder>
    </appender>

    <!-- Insert Job Logger Setup -->
    <logger name="insert-job" level="ERROR" additivity="false">
        <appender-ref ref="INSERT_FILE"/>
    </logger>

    <!-- Read Job Logger Setup -->
    <logger name="read-job" level="ERROR" additivity="false">
        <appender-ref ref="READ_FILE"/>
    </logger>

    <!-- Root logger (optional) -->
    <root level="INFO">
        <!-- You can add a general appender if needed -->
    </root>

</configuration>


import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.io.TextIO;
import org.apache.beam.sdk.testing.PAssert;
import org.apache.beam.sdk.testing.TestPipeline;
import org.apache.beam.sdk.transforms.Create;
import org.apache.beam.sdk.transforms.Filter;
import org.apache.beam.sdk.transforms.MapElements;
import org.apache.beam.sdk.values.PCollection;
import org.apache.beam.sdk.values.TypeDescriptor;
import org.junit.Before;
import org.junit.Rule;
import org.junit.Test;
import org.junit.rules.TemporaryFolder;

import java.io.File;
import java.io.FileWriter;
import java.util.Arrays;
import java.util.List;

import static org.junit.Assert.*;
import static org.mockito.Mockito.*;

public class dfUkrbRepeatUseDailyNormTest {

    @Rule
    public final transient TestPipeline pipeline = TestPipeline.create();

    @Rule
    public TemporaryFolder tempFolder = new TemporaryFolder();

    private List<String> inputLines;

    @Before
    public void setup() {
        inputLines = Arrays.asList(
            "HEADER line1",
            "BODY line2",
            "TRAILER line3"
        );
    }

    @Test
    public void testPipelineHeaderParsing() {
        PCollection<String> input = pipeline.apply(Create.of(inputLines));

        PCollection<String> headers = input.apply("Extract Headers", Filter.by(line -> line.startsWith("HEADER")));

        PAssert.that(headers).containsInAnyOrder("HEADER line1");
        pipeline.run().waitUntilFinish();
    }

    @Test
    public void testPipelineBodyParsing() {
        PCollection<String> input = pipeline.apply(Create.of(inputLines));

        PCollection<String> body = input.apply("Extract Body", Filter.by(line -> line.startsWith("BODY")));

        PAssert.that(body).containsInAnyOrder("BODY line2");
        pipeline.run().waitUntilFinish();
    }

    @Test
    public void testPipelineTrailerParsing() {
        PCollection<String> input = pipeline.apply(Create.of(inputLines));

        PCollection<String> trailer = input.apply("Extract Trailer", Filter.by(line -> line.startsWith("TRAILER")));

        PAssert.that(trailer).containsInAnyOrder("TRAILER line3");
        pipeline.run().waitUntilFinish();
    }

    @Test
    public void testMapToJobStatusSuccess() {
        Pipeline testPipe = TestPipeline.create();
        PCollection<JobStatus> jobStatus = PojoMapper.mapToJobStatus(testPipe, "testJob");

        PAssert.thatSingleton(jobStatus).satisfies(js -> {
            assertNotNull(js.getKey());
            assertNotNull(js.getMarket());
            return null;
        });

        testPipe.run().waitUntilFinish();
    }

    @Test
    public void testInitializeJob_noExceptionThrown() {
        DatabaseOperationsUtility dbUtil = mock(DatabaseOperationsUtility.class);

        try {
            doNothing().when(dbUtil).initializeJob(any(), any(), any());
            dbUtil.initializeJob(mock(Pipeline.class), "job", "log");
            verify(dbUtil, times(1)).initializeJob(any(), any(), any());
        } catch (Exception e) {
            fail("Initialization threw an exception");
        }
    }

    @Test
    public void testMainMethodSuccessfulRun() {
        try {
            dfUkrbRepeatUseDailyNorm.main(new String[]{});
        } catch (Exception e) {
            fail("Main should not throw exception: " + e.getMessage());
        }
    }

    @Test
    public void testMainMethodWithFailure_shouldLogAndExit() {
        // Simulate runtime failure by setting a condition in code (e.g. System property or mock)
        System.setProperty("FAIL_MAIN", "true");

        try {
            dfUkrbRepeatUseDailyNorm.main(new String[]{});
            fail("Expected an exception to be thrown");
        } catch (Exception e) {
            assertTrue(e.getMessage().contains("Runtime Error"));
        } finally {
            System.clearProperty("FAIL_MAIN");
        }
    }

    @Test
    public void testFileOutputCreation() throws Exception {
        File outFile = tempFolder.newFile("output.txt");

        FileWriter writer = new FileWriter(outFile);
        writer.write("SAMPLE OUTPUT");
        writer.close();

        assertTrue(outFile.exists());
        assertTrue(outFile.length() > 0);
    }
}


import org.apache.beam.sdk.testing.TestPipeline;
import org.apache.beam.sdk.testing.PAssert;
import org.apache.beam.sdk.transforms.Create;
import org.apache.beam.sdk.values.PCollection;
import org.junit.Rule;
import org.junit.Test;

import java.util.Arrays;
import java.util.List;

public class dfUkrbRepeatUseDailyNormTest {

    @Rule
    public final transient TestPipeline pipeline = TestPipeline.create();

    @Test
    public void testPipelineExecutionWithMockData() {
        // Mock input
        List<String> inputLines = Arrays.asList(
            "HEADER_LINE",
            "BODY_RECORD_1",
            "TRAILER_LINE"
        );

        PCollection<String> input = pipeline.apply("Read Mock Input", Create.of(inputLines));

        // Apply a basic transformation for test (simulate part of real logic)
        PCollection<String> headers = input.apply("Filter Headers", Filter.by(line -> line.startsWith("HEADER")));
        PCollection<String> trailers = input.apply("Filter Trailers", Filter.by(line -> line.startsWith("TRAILER")));

        // Verify filtered results
        PAssert.that(headers).containsInAnyOrder("HEADER_LINE");
        PAssert.that(trailers).containsInAnyOrder("TRAILER_LINE");

        pipeline.run().waitUntilFinish();
    }
}



import org.apache.avro.Schema;
import org.apache.avro.generic.GenericData;
import org.apache.avro.generic.GenericRecord;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.junit.jupiter.api.Test;
import org.apache.parquet.avro.AvroParquetReader;
import org.apache.parquet.avro.AvroParquetWriter;
import org.apache.parquet.hadoop.ParquetReader;
import org.apache.parquet.hadoop.ParquetWriter;
import org.apache.parquet.hadoop.metadata.CompressionCodecName;
import org.apache.parquet.hadoop.util.HadoopOutputFile;

import java.io.File;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

import static org.junit.jupiter.api.Assertions.assertEquals;

public class ParquetWriteReadTest {

    @Test
    public void testWriteAndReadParquetFile() throws IOException {
        // Define Avro schema
        String avroSchema = "{\"type\":\"record\",\"name\":\"TestRecord\",\"fields\":[{\"name\":\"id\",\"type\":\"int\"},{\"name\":\"name\",\"type\":\"string\"}]}";
        Schema schema = new Schema.Parser().parse(avroSchema);

        // Prepare test data
        List<GenericRecord> data = new ArrayList<>();
        GenericRecord record1 = new GenericData.Record(schema);
        record1.put("id", 1);
        record1.put("name", "Record 1");
        data.add(record1);

        GenericRecord record2 = new GenericData.Record(schema);
        record2.put("id", 2);
        record2.put("name", "Record 2");
        data.add(record2);

        // Write data to Parquet file
        File testFile = new File("test.parquet");
        Path path = new Path(testFile.getAbsolutePath());
       
        ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(HadoopOutputFile.fromPath(path, new Configuration()))
                .withSchema(schema)
                .withCompressionCodec(CompressionCodecName.SNAPPY)
                .build();
        for (GenericRecord record : data) {
            writer.write(record);
        }
        writer.close();

        // Read data from Parquet file
        ParquetReader<GenericRecord> reader = AvroParquetReader.<GenericRecord>builder(path).build();
        GenericRecord readRecord1 = reader.read();
        GenericRecord readRecord2 = reader.read();
        reader.close();

        // Assert data
        assertEquals(record1, readRecord1);
        assertEquals(record2, readRecord2);
    }
}
